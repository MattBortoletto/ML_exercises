{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clothes Classification with Support Vector Machines\n",
    "\n",
    "In this notebook we are going to explore the use of Support Vector Machines (SVM) for image classification. We are going to use a new version of the famous MNIST dataset (the original is a dataset of handwritten digits). The version we are going to use is called Fashion MNIST (https://pravarmahajan.github.io/fashion/) and is a dataset of small images of clothes and accessories.\n",
    "\n",
    "\n",
    "\n",
    "The dataset labels are the following:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Insert your surname, name and ID number\n",
    "\n",
    "Student name: Bortoletto Matteo\n",
    "    \n",
    "ID: 1146169 (this is my old ID, I'm graduating this december)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the required packages\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load Fashion MNIST dataset\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte.gz' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte.gz' % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix your ID (\"numero di matricola\") and the seed for random generator (as usual you can try different seeds)\n",
    "ID = 1146169 #Place your ID/seed\n",
    "np.random.seed(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n"
     ]
    }
   ],
   "source": [
    "#load the Fashion MNIST dataset from the 'data' folder and let's normalize the features so that each value is in [0,1] \n",
    "\n",
    "X, y = load_mnist('data', kind='train')\n",
    "# rescale the data\n",
    "X, y = X / 255., y # original pixel values are between 0 and 255\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split into training and test. Make sure that each label is present at least 10 times\n",
    "in training. If it is not, then keep adding permutations to the initial data until this \n",
    "happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in training dataset:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Frequencies in training dataset:  [47 63 52 54 48 48 45 49 46 48]\n"
     ]
    }
   ],
   "source": [
    "#random permute the data and split into training and test taking the first 500\n",
    "#data samples as training and the rests as test\n",
    "permutation = np.random.permutation(X.shape[0])\n",
    "\n",
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 500\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for plotting a image and printing the corresponding label\n",
    "def plot_input(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(\"LABEL: %i\"%labels[index])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 2\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 8\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 7\n",
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 6\n"
     ]
    }
   ],
   "source": [
    "#let's try the plotting function\n",
    "plot_input(X_train,y_train,5)\n",
    "plot_input(X_test,y_test,50)\n",
    "plot_input(X_test,y_test,500)\n",
    "plot_input(X_test,y_test,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 1\n",
    "Use a SVM classifier with cross validation to pick a model. Use a 4-fold cross-validation. Let's start with a Linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR LINEAR KERNEL\n",
      "Best parameters set found: {'C': 0.05}\n",
      "Score with best parameters: 0.776\n",
      "All scores on the grid:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.195701</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.039466</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>{'C': 0.0005}</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>0.341270</td>\n",
       "      <td>0.362903</td>\n",
       "      <td>0.360656</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.023963</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.113721</td>\n",
       "      <td>0.002035</td>\n",
       "      <td>0.045222</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'C': 0.005}</td>\n",
       "      <td>0.726562</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072120</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.05}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.022232</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.077893</td>\n",
       "      <td>0.002811</td>\n",
       "      <td>0.028176</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5}</td>\n",
       "      <td>0.773438</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.019955</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075050</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.026640</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 5}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.075332</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.026905</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>50</td>\n",
       "      <td>{'C': 50}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.074873</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.026641</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>500</td>\n",
       "      <td>{'C': 500}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>0.737705</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.020774</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.195701      0.005736         0.039466        0.003092  0.0005   \n",
       "1       0.113721      0.002035         0.045222        0.005563   0.005   \n",
       "2       0.072120      0.000878         0.026613        0.000398    0.05   \n",
       "3       0.077893      0.002811         0.028176        0.001668     0.5   \n",
       "4       0.075050      0.000885         0.026640        0.000517       5   \n",
       "5       0.075332      0.000814         0.026905        0.000849      50   \n",
       "6       0.074873      0.000727         0.026641        0.000319     500   \n",
       "\n",
       "          params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'C': 0.0005}           0.406250           0.341270           0.362903   \n",
       "1   {'C': 0.005}           0.726562           0.714286           0.701613   \n",
       "2    {'C': 0.05}           0.781250           0.793651           0.790323   \n",
       "3     {'C': 0.5}           0.773438           0.793651           0.766129   \n",
       "4       {'C': 5}           0.781250           0.793651           0.766129   \n",
       "5      {'C': 50}           0.781250           0.793651           0.766129   \n",
       "6     {'C': 500}           0.781250           0.793651           0.766129   \n",
       "\n",
       "   split3_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0           0.360656            0.368        0.023963                7  \n",
       "1           0.704918            0.712        0.009726                6  \n",
       "2           0.737705            0.776        0.022232                1  \n",
       "3           0.737705            0.768        0.019955                5  \n",
       "4           0.737705            0.770        0.020774                2  \n",
       "5           0.737705            0.770        0.020774                2  \n",
       "6           0.737705            0.770        0.020774                2  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "\n",
    "# parameters for linear SVM\n",
    "parameters = {'C': [0.0005, 0.005, 0.05, 0.5, 5, 50, 500]}\n",
    "\n",
    "#run linear SVM\n",
    "\n",
    "#ADD YOUR CODE \n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "classifier = GridSearchCV(svc, parameters, cv=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR LINEAR KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\", classifier.best_params_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"Score with best parameters:\", classifier.best_score_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"All scores on the grid:\" )\n",
    "#ADD YOUR CODE\n",
    "all_scores = pd.DataFrame(classifier.cv_results_)\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "Pick a model for the Polynomial kernel with degree=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR POLY DEGREE=2 KERNEL\n",
      "Best parameters set found: {'C': 0.05, 'gamma': 0.5}\n",
      "Score with best parameters: 0.766\n",
      "\n",
      "All scores on the grid:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.082618</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.05, 'gamma': 0.05}</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.030922</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069636</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.024896</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.05, 'gamma': 0.5}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071851</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.026931</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.05, 'gamma': 5.0}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.070479</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.025178</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.05}</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.027105</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069789</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.024918</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.5}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.070122</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>0.024960</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 5.0}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.069636</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024857</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 5, 'gamma': 0.05}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.069629</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.024936</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 5, 'gamma': 0.5}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.069615</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.024919</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 5, 'gamma': 5.0}</td>\n",
       "      <td>0.789062</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.729508</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.021942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.082618      0.003121         0.027435        0.000488    0.05   \n",
       "1       0.069636      0.000503         0.024896        0.000270    0.05   \n",
       "2       0.071851      0.002032         0.026931        0.002925    0.05   \n",
       "3       0.070479      0.001538         0.025178        0.000329     0.5   \n",
       "4       0.069789      0.000539         0.024918        0.000288     0.5   \n",
       "5       0.070122      0.000294         0.024960        0.000351     0.5   \n",
       "6       0.069636      0.000500         0.024857        0.000274       5   \n",
       "7       0.069629      0.000458         0.024936        0.000386       5   \n",
       "8       0.069615      0.000415         0.024919        0.000217       5   \n",
       "\n",
       "  param_gamma                      params  split0_test_score  \\\n",
       "0        0.05  {'C': 0.05, 'gamma': 0.05}           0.742188   \n",
       "1         0.5   {'C': 0.05, 'gamma': 0.5}           0.789062   \n",
       "2           5   {'C': 0.05, 'gamma': 5.0}           0.789062   \n",
       "3        0.05   {'C': 0.5, 'gamma': 0.05}           0.796875   \n",
       "4         0.5    {'C': 0.5, 'gamma': 0.5}           0.789062   \n",
       "5           5    {'C': 0.5, 'gamma': 5.0}           0.789062   \n",
       "6        0.05     {'C': 5, 'gamma': 0.05}           0.789062   \n",
       "7         0.5      {'C': 5, 'gamma': 0.5}           0.789062   \n",
       "8           5      {'C': 5, 'gamma': 5.0}           0.789062   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.769841           0.709677           0.688525            0.728   \n",
       "1           0.769841           0.774194           0.729508            0.766   \n",
       "2           0.769841           0.774194           0.729508            0.766   \n",
       "3           0.769841           0.758065           0.721311            0.762   \n",
       "4           0.769841           0.774194           0.729508            0.766   \n",
       "5           0.769841           0.774194           0.729508            0.766   \n",
       "6           0.769841           0.774194           0.729508            0.766   \n",
       "7           0.769841           0.774194           0.729508            0.766   \n",
       "8           0.769841           0.774194           0.729508            0.766   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.030922                9  \n",
       "1        0.021942                1  \n",
       "2        0.021942                1  \n",
       "3        0.027105                8  \n",
       "4        0.021942                1  \n",
       "5        0.021942                1  \n",
       "6        0.021942                1  \n",
       "7        0.021942                1  \n",
       "8        0.021942                1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for poly with degree 2 kernel\n",
    "parameters = {'C': [0.05, 0.5, 5],'gamma':[0.05,0.5,5.]}\n",
    "\n",
    "#run SVM with poly of degree 2 kernel\n",
    "\n",
    "#ADD YOUR CODE \n",
    "\n",
    "svc = SVC(kernel='poly', degree = 2)\n",
    "classifier = GridSearchCV(svc, parameters, cv=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=2 KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\", classifier.best_params_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"Score with best parameters:\", classifier.best_score_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "#ADD YOUR CODE \n",
    "all_scores = pd.DataFrame(classifier.cv_results_)\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 3\n",
    "\n",
    "Now let's try a higher degree for the polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR POLY DEGREE= 3  KERNEL\n",
      "Best parameters set found: {'C': 0.05, 'gamma': 0.5}\n",
      "Score with best parameters: 0.724\n",
      "\n",
      "All scores on the grid:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075272</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>0.027875</td>\n",
       "      <td>0.003342</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.05, 'gamma': 0.05}</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.693548</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.024891</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069201</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.024627</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.05, 'gamma': 0.5}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069413</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.025481</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.05</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.05, 'gamma': 5.0}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068334</td>\n",
       "      <td>0.002019</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.05}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.069499</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.024381</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.5}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.079004</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.030566</td>\n",
       "      <td>0.005807</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 5.0}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.067670</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.023721</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 5, 'gamma': 0.05}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.069690</td>\n",
       "      <td>0.002044</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 5, 'gamma': 0.5}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.067648</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 5, 'gamma': 5.0}</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.685484</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.028044</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.075272      0.004078         0.027875        0.003342    0.05   \n",
       "1       0.069201      0.000814         0.024627        0.000539    0.05   \n",
       "2       0.069413      0.001915         0.025481        0.002761    0.05   \n",
       "3       0.068334      0.002019         0.024403        0.001116     0.5   \n",
       "4       0.069499      0.002247         0.024381        0.001293     0.5   \n",
       "5       0.079004      0.004009         0.030566        0.005807     0.5   \n",
       "6       0.067670      0.001131         0.023721        0.000802       5   \n",
       "7       0.069690      0.002044         0.024800        0.001569       5   \n",
       "8       0.067648      0.001361         0.023764        0.000821       5   \n",
       "\n",
       "  param_gamma                      params  split0_test_score  \\\n",
       "0        0.05  {'C': 0.05, 'gamma': 0.05}           0.718750   \n",
       "1         0.5   {'C': 0.05, 'gamma': 0.5}           0.734375   \n",
       "2           5   {'C': 0.05, 'gamma': 5.0}           0.734375   \n",
       "3        0.05   {'C': 0.5, 'gamma': 0.05}           0.734375   \n",
       "4         0.5    {'C': 0.5, 'gamma': 0.5}           0.734375   \n",
       "5           5    {'C': 0.5, 'gamma': 5.0}           0.734375   \n",
       "6        0.05     {'C': 5, 'gamma': 0.05}           0.734375   \n",
       "7         0.5      {'C': 5, 'gamma': 0.5}           0.734375   \n",
       "8           5      {'C': 5, 'gamma': 5.0}           0.734375   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0           0.738095           0.693548           0.672131            0.706   \n",
       "1           0.761905           0.685484           0.713115            0.724   \n",
       "2           0.761905           0.685484           0.713115            0.724   \n",
       "3           0.746032           0.685484           0.688525            0.714   \n",
       "4           0.761905           0.685484           0.713115            0.724   \n",
       "5           0.761905           0.685484           0.713115            0.724   \n",
       "6           0.761905           0.685484           0.713115            0.724   \n",
       "7           0.761905           0.685484           0.713115            0.724   \n",
       "8           0.761905           0.685484           0.713115            0.724   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        0.024891                9  \n",
       "1        0.028044                1  \n",
       "2        0.028044                1  \n",
       "3        0.026923                8  \n",
       "4        0.028044                1  \n",
       "5        0.028044                1  \n",
       "6        0.028044                1  \n",
       "7        0.028044                1  \n",
       "8        0.028044                1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for poly with higher degree kernel\n",
    "parameters = {'C': [0.05, 0.5, 5],'gamma':[0.05,0.5,5.]}\n",
    "\n",
    "#run SVM with poly of higher degree kernel\n",
    "degree = 3\n",
    "\n",
    "#ADD YOUR CODE \n",
    "\n",
    "svc = SVC(kernel='poly', degree=degree)\n",
    "classifier = GridSearchCV(svc, parameters, cv=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR POLY DEGREE=', degree, ' KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\", classifier.best_params_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"Score with best parameters:\", classifier.best_score_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "#ADD YOUR CODE \n",
    "all_scores = pd.DataFrame(classifier.cv_results_)\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "Pick a model for the Radial Basis Function kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR rbf KERNEL\n",
      "Best parameters set found: {'C': 50, 'gamma': 0.005}\n",
      "Score with best parameters: 0.79\n",
      "\n",
      "All scores on the grid:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.129636</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>0.035108</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.005}</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.713115</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205450</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.038198</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.05}</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.042740</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.214615</td>\n",
       "      <td>0.002780</td>\n",
       "      <td>0.037606</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 0.5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.213332</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.038841</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 0.5, 'gamma': 5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.089035</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.029585</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'C': 5, 'gamma': 0.005}</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.013001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.207373</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.035559</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 5, 'gamma': 0.05}</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.733871</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.221229</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.037525</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 5, 'gamma': 0.5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.214349</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.038751</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 5, 'gamma': 5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.094462</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>50</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'C': 50, 'gamma': 0.005}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.206964</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.035562</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 50, 'gamma': 0.05}</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.733871</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.220814</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.038056</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 50, 'gamma': 0.5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.214391</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.038829</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 50, 'gamma': 5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.092865</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>500</td>\n",
       "      <td>0.005</td>\n",
       "      <td>{'C': 500, 'gamma': 0.005}</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.208278</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.035479</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>500</td>\n",
       "      <td>0.05</td>\n",
       "      <td>{'C': 500, 'gamma': 0.05}</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.733871</td>\n",
       "      <td>0.704918</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.219551</td>\n",
       "      <td>0.002544</td>\n",
       "      <td>0.037536</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'C': 500, 'gamma': 0.5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.238820</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>500</td>\n",
       "      <td>5</td>\n",
       "      <td>{'C': 500, 'gamma': 5}</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.122951</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.129636      0.003965         0.035108        0.000492     0.5   \n",
       "1        0.205450      0.005441         0.038198        0.003836     0.5   \n",
       "2        0.214615      0.002780         0.037606        0.000340     0.5   \n",
       "3        0.213332      0.003511         0.038841        0.000355     0.5   \n",
       "4        0.089035      0.000748         0.029585        0.000784       5   \n",
       "5        0.207373      0.002379         0.035559        0.000581       5   \n",
       "6        0.221229      0.001644         0.037525        0.000599       5   \n",
       "7        0.214349      0.002514         0.038751        0.000594       5   \n",
       "8        0.094462      0.001901         0.029329        0.000564      50   \n",
       "9        0.206964      0.002278         0.035562        0.000519      50   \n",
       "10       0.220814      0.003507         0.038056        0.000339      50   \n",
       "11       0.214391      0.002395         0.038829        0.000487      50   \n",
       "12       0.092865      0.001472         0.029273        0.000626     500   \n",
       "13       0.208278      0.001968         0.035479        0.000496     500   \n",
       "14       0.219551      0.002544         0.037536        0.000249     500   \n",
       "15       0.238820      0.021121         0.048900        0.014131     500   \n",
       "\n",
       "   param_gamma                      params  split0_test_score  \\\n",
       "0        0.005  {'C': 0.5, 'gamma': 0.005}           0.703125   \n",
       "1         0.05   {'C': 0.5, 'gamma': 0.05}           0.609375   \n",
       "2          0.5    {'C': 0.5, 'gamma': 0.5}           0.125000   \n",
       "3            5      {'C': 0.5, 'gamma': 5}           0.125000   \n",
       "4        0.005    {'C': 5, 'gamma': 0.005}           0.765625   \n",
       "5         0.05     {'C': 5, 'gamma': 0.05}           0.742188   \n",
       "6          0.5      {'C': 5, 'gamma': 0.5}           0.125000   \n",
       "7            5        {'C': 5, 'gamma': 5}           0.125000   \n",
       "8        0.005   {'C': 50, 'gamma': 0.005}           0.781250   \n",
       "9         0.05    {'C': 50, 'gamma': 0.05}           0.742188   \n",
       "10         0.5     {'C': 50, 'gamma': 0.5}           0.125000   \n",
       "11           5       {'C': 50, 'gamma': 5}           0.125000   \n",
       "12       0.005  {'C': 500, 'gamma': 0.005}           0.781250   \n",
       "13        0.05   {'C': 500, 'gamma': 0.05}           0.742188   \n",
       "14         0.5    {'C': 500, 'gamma': 0.5}           0.125000   \n",
       "15           5      {'C': 500, 'gamma': 5}           0.125000   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  mean_test_score  \\\n",
       "0            0.714286           0.701613           0.713115            0.708   \n",
       "1            0.706349           0.596774           0.647541            0.640   \n",
       "2            0.126984           0.129032           0.122951            0.126   \n",
       "3            0.126984           0.129032           0.122951            0.126   \n",
       "4            0.769841           0.790323           0.754098            0.770   \n",
       "5            0.753968           0.733871           0.704918            0.734   \n",
       "6            0.126984           0.129032           0.122951            0.126   \n",
       "7            0.126984           0.129032           0.122951            0.126   \n",
       "8            0.801587           0.790323           0.786885            0.790   \n",
       "9            0.753968           0.733871           0.704918            0.734   \n",
       "10           0.126984           0.129032           0.122951            0.126   \n",
       "11           0.126984           0.129032           0.122951            0.126   \n",
       "12           0.801587           0.790323           0.786885            0.790   \n",
       "13           0.753968           0.733871           0.704918            0.734   \n",
       "14           0.126984           0.129032           0.122951            0.126   \n",
       "15           0.126984           0.129032           0.122951            0.126   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "0         0.005704                7  \n",
       "1         0.042740                8  \n",
       "2         0.002247                9  \n",
       "3         0.002247                9  \n",
       "4         0.013001                3  \n",
       "5         0.018000                4  \n",
       "6         0.002247                9  \n",
       "7         0.002247                9  \n",
       "8         0.007472                1  \n",
       "9         0.018000                4  \n",
       "10        0.002247                9  \n",
       "11        0.002247                9  \n",
       "12        0.007472                1  \n",
       "13        0.018000                4  \n",
       "14        0.002247                9  \n",
       "15        0.002247                9  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for rbf SVM\n",
    "parameters = {'C': [0.5, 5, 50, 500],'gamma':[0.005, 0.05, 0.5,5]}\n",
    "\n",
    "#run SVM with rbf kernel\n",
    "\n",
    "#ADD YOUR CODE \n",
    "\n",
    "svc = SVC(kernel='rbf')\n",
    "classifier = GridSearchCV(svc, parameters, cv=4)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print ('RESULTS FOR rbf KERNEL')\n",
    "\n",
    "print(\"Best parameters set found:\", classifier.best_params_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"Score with best parameters:\", classifier.best_score_)\n",
    "#ADD YOUR CODE \n",
    "\n",
    "print(\"\\nAll scores on the grid:\")\n",
    "#ADD YOUR CODE \n",
    "all_scores = pd.DataFrame(classifier.cv_results_)\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO5\n",
    "What do you observe when using RBF and polynomial kernels on this dataset ?\n",
    "\n",
    "### ANSWER TO THE QUESTION\n",
    "Using polynomial kernels the best parameter found is `C = 0.05` and the scores are `0.776` for the linear and the degree 2 kernels and `0.724` with degree 3 kernel. Using RBF kernel the best parameter found is `C = 50` and the score is `0.79`. So I can conclude that the best kernel is the RBF (because if has the higher score) and the worst is the degree 3 kernel (because it has the lower score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 6\n",
    "Report here the best SVM kernel and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM training error: 0.000000\n",
      "Best SVM test error: 0.230370\n"
     ]
    }
   ],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "best_SVM = SVC(kernel='rbf', gamma=0.05, C=50) #ADD YOUR CODE \n",
    "\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More data\n",
    "Now let's do the same but using more data points for training.\n",
    "\n",
    "\n",
    "Choose a new number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in training dataset:  [0 1 2 3 4 5 6 7 8 9]\n",
      "Frequencies in training dataset:  [183 222 212 209 168 205 204 218 192 187]\n"
     ]
    }
   ],
   "source": [
    "X = X[permutation]\n",
    "y = y[permutation]\n",
    "\n",
    "m_training = 2000 # TODO number of data points, adjust depending on the capabilities of your PC\n",
    "\n",
    "X_train, X_test = X[:m_training], X[m_training:]\n",
    "y_train, y_test = y[:m_training], y[m_training:]\n",
    "\n",
    "labels, freqs = np.unique(y_train, return_counts=True)\n",
    "print(\"Labels in training dataset: \", labels)\n",
    "print(\"Frequencies in training dataset: \", freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use SVM with parameters obtained from the best model for $m_{training} =  2000$. Since it may take a long time to run, you can decide to just let it run for some time and stop it if it does not complete. If you decide to do this, report it in the TO DO 9 cell below.\n",
    "\n",
    "### TO DO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SVM training error: 0.000000\n",
      "Best SVM test error: 0.178948\n"
     ]
    }
   ],
   "source": [
    "#get training and test error for the best SVM model from CV\n",
    "\n",
    "# ADD YOUR CODE\n",
    "best_SVM = SVC(kernel='rbf', gamma=0.05, C=50) #ADD YOUR CODE \n",
    "\n",
    "best_SVM.fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - best_SVM.score(X_train,y_train)\n",
    "test_error = 1. - best_SVM.score(X_test,y_test)\n",
    "\n",
    "print (\"Best SVM training error: %f\" % training_error)\n",
    "print (\"Best SVM test error: %f\" % test_error)\n",
    "\n",
    "# Results: \n",
    "# Best SVM training error: 0.000000\n",
    "# Best SVM test error: 0.178948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison, let's also use logistic regression (with standard parameters from scikit-learn, i.e. some regularization is included).\n",
    "\n",
    "### TO DO 8 Try first without regularization (use a very large large C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logistic regression training error: 0.000000\n",
      "Best logistic regression test error: 0.234259\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "### ADD YOUR CODE\n",
    "LogReg = linear_model.LogisticRegression(C = 1e5).fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - LogReg.score(X_train,y_train)\n",
    "test_error = 1. - LogReg.score(X_test,y_test)\n",
    "\n",
    "print (\"Best logistic regression training error: %f\" % training_error)\n",
    "print (\"Best logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO 9 Then use also some regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/borto/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularized logistic regression training error: 0.040500\n",
      "Best regularized logistic regression test error: 0.187241\n"
     ]
    }
   ],
   "source": [
    "### ADD YOUR CODE\n",
    "\n",
    "LogReg = linear_model.LogisticRegression(penalty='l2').fit(X_train, y_train)\n",
    "\n",
    "training_error = 1. - LogReg.score(X_train,y_train)\n",
    "test_error = 1. - LogReg.score(X_test,y_test)\n",
    "\n",
    "print (\"Best regularized logistic regression training error: %f\" % training_error)\n",
    "print (\"Best regularized logistic regression test error: %f\" % test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 10\n",
    "Compare and discuss:\n",
    "- the results from SVM with m=500 and with m=2000 training data points. If you stopped the SVM, include such aspect in your comparison.\n",
    "- the results of SVM and of Logistic Regression with and without regularization\n",
    "\n",
    "### ANSWER TO THE QUESTION\n",
    "Using the SVM with m = 500 the best training error is `0` and the best test error is `0.230370`. Using the SVM with m = 2000 (without interrupting it) the best training error is `0` and the best test error is `0.178948`, which is lower than the test error obtained with m = 500. This means that using more training data the algorithm improves. <br>\n",
    "Using the logistic regression without regularization the best train error is `0` and the best test error is `0.234259` whereas using the regularized logistic regression the best train error is `0.040500` and the best test error is `0.187241`. These results are very similar with the SVM with m = 500 and the SVM with m = 2000, respectively. Note also that the case of regularized logistic regression is the only in which the best train error is not `0`, but it is still very small. So, we can say that the SVM with m = 2000 and the regularized logistic regression are the best solutions for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 10\n",
    "Plot an item of clothing that is missclassified by logistic regression and correctly classified by SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: 4\n",
      "LR_prediction: 2 SMV_prediction 4\n"
     ]
    }
   ],
   "source": [
    "LR_prediction = LogReg.predict(X_test) # ADD CODE\n",
    "SVM_prediction = best_SVM.predict(X_test) # ADD CODE\n",
    "\n",
    "# ADD CODE\n",
    "\n",
    "#misscl_index = -1\n",
    "\n",
    "# for i in range(len(y_test)):\n",
    "#    if (SVM_prediction[i] == y_test[i]) & (LR_prediction[i] != y_test[i]):\n",
    "#        misscl_index = i\n",
    "#    break\n",
    "\n",
    "misscl_index = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    if (SVM_prediction[i] == y_test[i]) and (LR_prediction[i] != y_test[i]):\n",
    "        misscl_index.append(i)\n",
    "\n",
    "plot_input(X_test, y_test, misscl_index[0])\n",
    "\n",
    "print('LR_prediction:', LR_prediction[misscl_index[0]], 'SMV_prediction', SVM_prediction[misscl_index[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 11\n",
    "Plot the confusion matrix for the SVM classifier and for logistic regression.\n",
    "The confusion matrix has one column for each predicted label and one row for each true label. \n",
    "It shows for each class in the corresponding row how many samples belonging to that class gets each possible output label.\n",
    "Notice that the diagonal contains the correctly classified samples, while the other cells correspond to errors.\n",
    "You can obtain it with the sklearn.metrics.confusion_matrix function (see the documentation).\n",
    "Try also to normalize the confusion matrix by the number of samples in each class in order to measure the accuracy on each single class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels and frequencies in test set:  [5817 5778 5788 5791 5832 5795 5796 5782 5808 5813]\n",
      "\n",
      " Confusion matrix SVM  \n",
      " \n",
      " [[4534    0  126  217   15    8  701    0  215    1]\n",
      " [  52 5349  109  150    6    0   70    0   42    0]\n",
      " [  59    0 4405   44  608    8  459    0  205    0]\n",
      " [ 308   29   70 4957  132    0  213    0   82    0]\n",
      " [  21    4  748  289 4085    3  555    0  127    0]\n",
      " [   0    0    0    4    0 5033    3  190  449  116]\n",
      " [ 898    5  774  133  440    9 3300    0  237    0]\n",
      " [   0    0    0    0    0  263    0 5221   27  271]\n",
      " [   6    0   42   36   14  161   74   30 5440    5]\n",
      " [   0    0    0    1    0  107    3  332   73 5297]]\n",
      "\n",
      " Confusion matrix SVM (normalized)   \n",
      " \n",
      " [[0.78 0.   0.02 0.04 0.   0.   0.12 0.   0.04 0.  ]\n",
      " [0.01 0.93 0.02 0.03 0.   0.   0.01 0.   0.01 0.  ]\n",
      " [0.01 0.   0.76 0.01 0.11 0.   0.08 0.   0.04 0.  ]\n",
      " [0.05 0.01 0.01 0.86 0.02 0.   0.04 0.   0.01 0.  ]\n",
      " [0.   0.   0.13 0.05 0.7  0.   0.1  0.   0.02 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.87 0.   0.03 0.08 0.02]\n",
      " [0.15 0.   0.13 0.02 0.08 0.   0.57 0.   0.04 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.05 0.   0.9  0.   0.05]\n",
      " [0.   0.   0.01 0.01 0.   0.03 0.01 0.01 0.94 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.02 0.   0.06 0.01 0.91]]\n",
      "\n",
      " Confusion matrix LR  \n",
      " \n",
      " [[4459   31  196  245   32    9  753    2   86    4]\n",
      " [  31 5430   84  156   17    1   54    2    2    1]\n",
      " [ 160    6 4200   69  811    7  472    1   60    2]\n",
      " [ 354   89  165 4790  155    0  214    2   21    1]\n",
      " [  33   10  659  300 4163    2  611    0   53    1]\n",
      " [   4    2    9   27    4 5099   10  364   61  215]\n",
      " [ 925   12  785  180  622    8 3123    1  139    1]\n",
      " [   0    0    0    1    0  303    1 5214    8  255]\n",
      " [  36    4   49   88   38   56  139   53 5339    6]\n",
      " [   0    0    1    5    1  133    7  338    5 5323]]\n",
      "\n",
      " Confusion matrix LR (normalized)   \n",
      " \n",
      " [[0.77 0.01 0.03 0.04 0.01 0.   0.13 0.   0.01 0.  ]\n",
      " [0.01 0.94 0.01 0.03 0.   0.   0.01 0.   0.   0.  ]\n",
      " [0.03 0.   0.73 0.01 0.14 0.   0.08 0.   0.01 0.  ]\n",
      " [0.06 0.02 0.03 0.83 0.03 0.   0.04 0.   0.   0.  ]\n",
      " [0.01 0.   0.11 0.05 0.71 0.   0.1  0.   0.01 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.88 0.   0.06 0.01 0.04]\n",
      " [0.16 0.   0.14 0.03 0.11 0.   0.54 0.   0.02 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.05 0.   0.9  0.   0.04]\n",
      " [0.01 0.   0.01 0.02 0.01 0.01 0.02 0.01 0.92 0.  ]\n",
      " [0.   0.   0.   0.   0.   0.02 0.   0.06 0.   0.92]]\n"
     ]
    }
   ],
   "source": [
    "# for better aligned printing of confusion matrix use floatmode='fixed' (not supported in all versions of Python)\n",
    "np.set_printoptions(precision=2, suppress=True) \n",
    "\n",
    "u, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Labels and frequencies in test set: \", counts)\n",
    "\n",
    "confusion_SVM = skm.confusion_matrix(y_test, SVM_prediction) #ADD CODE\n",
    "print(\"\\n Confusion matrix SVM  \\n \\n\", confusion_SVM)\n",
    "print(\"\\n Confusion matrix SVM (normalized)   \\n \\n\", confusion_SVM /counts[:,None] )\n",
    "\n",
    "confusion_LR = skm.confusion_matrix(y_test, LR_prediction) #ADD CODE\n",
    "print(\"\\n Confusion matrix LR  \\n \\n\", confusion_LR)\n",
    "print(\"\\n Confusion matrix LR (normalized)   \\n \\n\", confusion_LR /counts[:,None] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD CODE TO NORMALIZE CONFUSION MATRIX AND PRINT THE NORMALIZED MATRIX\n",
    "\n",
    "#norm_conf_SVM = confusion_SVM.astype(float)\n",
    "#norm_conf_LR = confusion_LR.astype(float)\n",
    "\n",
    "#for i in range(norm_conf_SVM[:,0]):\n",
    "#    sum_SVM = confusion_SVM[i,:].sum()\n",
    "#    sum_LR = confusion_LR[i,:].sum()\n",
    "\n",
    "#norm_conf_SVM = confusion_SVM / sum_SVM\n",
    "#norm_conf_LR = confusion_LR / sum_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 12\n",
    "Have a look at the confusion matrices and comment on the obtained accuracies. Why some classes have lower accuracies and others an higher one ? Make some guesses on the possible causes.\n",
    "\n",
    "### ANSWER TO THE QUESTION\n",
    "The results are pretty good, in fact the diagonal terms are close to 1 (except for the `shirt` one, which is 0.54). Some classes have lower accuracies and others an higher one because some clothes are very similar, so it is easier to confuse them, whereas others are very different, so it's difficult to confuse them. Indeed, `shirt` can be easily confused with `coat`, `pullover` or `t-shirt`, so this is why the diagonal term is smaller than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
